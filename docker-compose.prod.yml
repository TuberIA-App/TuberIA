# ============================================
# Docker Compose - Production Configuration
# For VPS deployment with Caddy reverse proxy
# ============================================
#
# Usage:
#   docker compose -f docker-compose.yml -f docker-compose.prod.yml up -d
#
# Prerequisites:
#   1. Create secrets files in ./secrets/ directory
#   2. Set DOMAIN_NAME environment variable or update labels below
#   3. Ensure MongoDB Atlas or managed MongoDB is configured
# ============================================

services:

  # Caddy Reverse Proxy with automatic SSL
  caddy:
    image: caddy:2-alpine
    container_name: caddy
    restart: always

    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"  # HTTP/3 support

    volumes:
      # Caddyfile configuration (read-only)
      - ./Caddyfile:/etc/caddy/Caddyfile:ro

      # Persistent certificate storage
      - caddy_data:/data
      - caddy_config:/config

      # Logs directory (create with: mkdir -p logs/caddy)
      - ./logs/caddy:/var/log/caddy

    networks:
      - tuberia-network

    environment:
      - DOMAIN_NAME=${DOMAIN_NAME:-localhost}
      - SSL_EMAIL=${SSL_EMAIL:-admin@example.com}

    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://backend:5000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Frontend - Production build served by Nginx
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: production  # Use production stage
      args:
        # Use relative path - Caddy routes /api to backend
        VITE_API_URL: /api

    container_name: tuberia-frontend-prod
    restart: always

    # No direct port exposure - accessed via Caddy
    expose:
      - "3000"

    # Production environment variables
    environment:
      - NODE_ENV=production

    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          memory: 128M

    networks:
      - tuberia-network

    depends_on:
      backend:
        condition: service_healthy

  # Backend - Production Node.js application
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production  # Use production stage

    container_name: tuberia-backend-prod
    restart: always

    # No direct port exposure - accessed via Caddy
    expose:
      - "5000"

    # Production environment variables
    environment:
      - NODE_ENV=production
      - PORT=5000
      - MONGODB_URI=mongodb://mongo:mongo@mongo:27017/tuberia?authSource=admin
      - FRONTEND_URL=https://${DOMAIN_NAME:-localhost}
      - LOG_LEVEL=info
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - WORKER_CONCURRENCY=2
      - RSS_POLL_INTERVAL=30

    # Docker secrets for sensitive data (read from /run/secrets/ in container)
    secrets:
      - jwt_secret
      - jwt_refresh_secret
      - openrouter_api_key

    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          memory: 256M

    networks:
      - tuberia-network
      - backend-internal

    depends_on:
      mongo:
        condition: service_healthy
      redis:
        condition: service_healthy

    healthcheck:
      test: |
        node -e "
        const http = require('http');
        http.get('http://localhost:5000/health', (res) => {
          let data = '';
          res.on('data', chunk => data += chunk);
          res.on('end', () => {
            try {
              const health = JSON.parse(data);
              const allHealthy = health.status === 'ok' &&
                                 health.services.redis === 'healthy' &&
                                 health.services.mongodb === 'healthy';
              process.exit(allHealthy ? 0 : 1);
            } catch {
              process.exit(1);
            }
          });
        }).on('error', () => process.exit(1));
        "
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # MongoDB - Production (containerized - NOT RECOMMENDED for production)
  # For production, use MongoDB Atlas or managed service instead
  mongo:
    image: mongo:7.0
    container_name: mongodb-prod
    restart: always

    # NO PORT EXPOSURE - internal network only
    expose:
      - "27017"

    # Credentials via environment (use secrets in real production)
    environment:
      - MONGO_INITDB_ROOT_USERNAME=mongo
      - MONGO_INITDB_ROOT_PASSWORD=mongo
      - MONGO_INITDB_DATABASE=tuberia_db

    # Persistent volume
    volumes:
      - mongo_data_prod:/data/db
      - ./mongo-backups:/backups  # For manual backups

    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          memory: 512M

    # Internal network only
    networks:
      - backend-internal

    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  # Redis - Production (BullMQ + Caching)
  redis:
    image: redis:7-alpine
    container_name: tuberia-redis-prod
    restart: always

    # NO PORT EXPOSURE - internal network only
    expose:
      - "6379"

    # Production command with memory limits and noeviction policy for BullMQ
    command: >
      redis-server
      --maxmemory 128mb
      --maxmemory-policy noeviction
      --save 60 1000
      --save 300 10
      --save 900 1
      --appendonly yes
      --appendfsync everysec

    # Persistent volume for job queue data
    volumes:
      - redis_data_prod:/data

    # Resource limits (critical for 2GB VPS)
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          memory: 64M

    # Internal network only (no public access)
    networks:
      - backend-internal

    # Health check
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=redis"


# Docker Secrets for sensitive data
secrets:
  jwt_secret:
    file: ./secrets/jwt_secret.txt
  jwt_refresh_secret:
    file: ./secrets/jwt_refresh_secret.txt
  openrouter_api_key:
    file: ./secrets/openrouter_api_key.txt

# Volumes
volumes:
  mongo_data_prod:
    driver: local

  # Redis persistent storage
  redis_data_prod:
    driver: local

  caddy_data:
    driver: local

  # Caddy config volume (stores configuration cache)
  caddy_config:
    driver: local

# Networks
networks:
  tuberia-network:
    driver: bridge
  backend-internal:
    driver: bridge
    internal: true  # No external access - backend and mongo only
